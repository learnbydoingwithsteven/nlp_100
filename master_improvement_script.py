"""
Master App Improvement Script
Systematically improves all 100 NLP apps with proper implementations
This script creates complete, functional app.py files with real NLP functionality
"""

import os
from pathlib import Path
import shutil

BASE_DIR = Path(__file__).parent

def create_improved_app(app_num, title, emoji, use_case, imports, function_code, demo_texts, requirements_add=None):
    """
    Create or update an app with proper NLP functionality
    
    Args:
        app_num: App number (1-100)
        title: App title
        emoji: Emoji for the app
        use_case: Description of the real-world use case  
        imports: Additional imports needed
        function_code: The main NLP processing function
        demo_texts: List of demo text examples
        requirements_add: Additional requirements to add
    """
    
    # Determine folder name
    folder_name = f"app_{app_num:03d}_{title.lower().replace(' ', '_').replace('/', '_')}"
    app_dir = BASE_DIR / folder_name
    
    if not app_dir.exists():
        print(f"‚ö†Ô∏è  Warning: {folder_name} does not exist!")
        return False
    
    app_file = app_dir / "app.py"
    
    # Build the complete app content
    app_content = f'''"""
NLP App {app_num:03d}: {title}
Real-world use case: {use_case}
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
{imports}

st.set_page_config(
    page_title="{title}",
    page_icon="{emoji}",
    layout="wide"
)

st.title("{emoji} {title}")
st.markdown("""
**Real-world Use Case**: {use_case}
- Analyze and process text with real NLP techniques
- Extract meaningful insights and patterns
- Visualize results comprehensively
""")

# Sidebar
st.sidebar.header("‚öôÔ∏è Configuration")
mode = st.sidebar.selectbox("Mode", ["Single Input", "Batch Processing", "Demo"])

{function_code}

# Mode: Single Input
if mode == "Single Input":
    st.header("üìù Single Text Processing")
    
    user_input = st.text_area(
        "Enter text to process:",
        height=150,
        placeholder="Type or paste your text here..."
    )
    
    if st.button("üîç Analyze", type="primary"):
        if user_input.strip():
            with st.spinner("Processing..."):
                result = process_text(user_input)
            
            st.success("‚úÖ Analysis Complete!")
            
            # Display key metrics
            metrics_cols = st.columns(min(4, len(result)))
            metric_keys = list(result.keys())[:4]
            
            for idx, key in enumerate(metric_keys):
                if key != 'text' and isinstance(result[key], (int, float, str)):
                    with metrics_cols[idx]:
                        value = result[key]
                        if isinstance(value, float):
                            st.metric(key.replace('_', ' ').title(), f"{{{{value:.3f}}}}")
                        else:
                            st.metric(key.replace('_', ' ').title(), value)
            
            # Detailed results
            st.subheader("üìä Detailed Results")
            
            # Create two columns for display
            col1, col2 = st.columns(2)
            
            with col1:
                st.json({{k: v for k, v in result.items() if k != 'text' and not k.startswith('_')}})
            
            with col2:
                # Visualization if numeric data available
                numeric_keys = [k for k, v in result.items() if isinstance(v, (int, float)) and k not in ['length', 'word_count']]
                if numeric_keys:
                    values = [result[k] for k in numeric_keys]
                    fig = px.bar(x=numeric_keys, y=values, title='Analysis Metrics')
                    st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("Please enter some text to analyze.")

# Mode: Batch Processing
elif mode == "Batch Processing":
    st.header("üìö Batch Processing")
    
    uploaded_file = st.file_uploader("Upload CSV with 'text' column", type=['csv'])
    
    if uploaded_file:
        df = pd.read_csv(uploaded_file)
        st.write(f"Loaded {{{{len(df)}}}} rows")
        
        if 'text' in df.columns:
            if st.button("üîç Process All", type="primary"):
                results = []
                progress_bar = st.progress(0)
                
                for idx, text in enumerate(df['text']):
                    result = process_text(str(text))
                    results.append(result)
                    progress_bar.progress((idx + 1) / len(df))
                
                results_df = pd.DataFrame(results)
                st.success(f"‚úÖ Processed {{{{len(results_df)}}}} texts!")
                
                # Summary stats
                st.subheader("üìä Summary Statistics")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Total Processed", len(results_df))
                with col2:
                    if 'word_count' in results_df.columns:
                        st.metric("Avg Word Count", f"{{{{results_df['word_count'].mean():.1f}}}}")
                with col3:
                    numeric_cols = results_df.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) > 0:
                        st.metric("Total Metrics", len(numeric_cols))
                
                # Visualizations
                numeric_cols = results_df.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 0:
                    col_to_plot = numeric_cols[0]
                    fig = px.histogram(results_df, x=col_to_plot, title=f'{{{{col_to_plot.replace("_", " ").title()}}}} Distribution')
                    st.plotly_chart(fig, use_container_width=True)
                
                # Results table
                st.subheader("üìã Detailed Results")
                st.dataframe(results_df, use_container_width=True)
                
                # Download
                csv = results_df.to_csv(index=False)
                st.download_button("üì• Download Results", csv, "results.csv", "text/csv")
        else:
            st.error("CSV must contain 'text' column")
    else:
        st.info("Upload a CSV file to perform batch processing")

# Mode: Demo
else:
    st.header("üéØ Demo Mode")
    
    sample_texts = {demo_texts}
    
    st.write(f"Processing {{{{len(sample_texts)}}}} sample texts...")
    
    # Display sample texts
    with st.expander("üëÅÔ∏è View Sample Texts"):
        for i, text in enumerate(sample_texts, 1):
            st.write(f"{{{{i}}}}. {{{{text}}}}")
    
    if st.button("üöÄ Run Demo", type="primary"):
        results = []
        with st.spinner("Processing..."):
            for text in sample_texts:
                result = process_text(text)
                results.append(result)
        
        results_df = pd.DataFrame(results)
        
        st.success("‚úÖ Demo Complete!")
        
        # Display results
        st.subheader("üìã Analysis Results")
        for idx, row in results_df.iterrows():
            with st.expander(f"üìÑ Sample {{{{idx+1}}}}"):
                text_preview = row['text'][:100] + ('...' if len(row['text']) > 100 else '')
                st.write(f"**Text**: {{{{text_preview}}}}")
                
                # Display key metrics
                metric_cols = st.columns(min(4, len(row) - 1))
                displayed = 0
                for key, value in row.items():
                    if key != 'text' and displayed < 4 and isinstance(value, (int, float, str)):
                        with metric_cols[displayed]:
                            if isinstance(value, float):
                                st.metric(key.replace('_', ' ').title(), f"{{{{value:.3f}}}}")
                            else:
                                st.metric(key.replace('_', ' ').title(), value)
                        displayed += 1
        
        # Visualizations
        st.subheader("üìä Results Visualization")
        numeric_cols = results_df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            col_to_plot = numeric_cols[0]
            fig = px.bar(
                results_df.reset_index(),
                x='index',
                y=col_to_plot,
                title=f'{{{{col_to_plot.replace("_", " ").title()}}}} by Sample',
                labels={{'index': 'Sample #', col_to_plot: col_to_plot.replace("_", " ").title()}}
            )
            st.plotly_chart(fig, use_container_width=True)

st.markdown("---")
st.markdown(f"**About**: {title} - {use_case}")
'''
    
    # Write the file
    try:
        # Backup original
        if app_file.exists():
            backup_file = app_dir / "app_original_backup.py"
            if not backup_file.exists():
                shutil.copy(app_file, backup_file)
        
        with open(app_file, 'w', encoding='utf-8') as f:
            f.write(app_content)
        
        print(f"‚úÖ App {app_num:03d} ({title}) improved successfully!")
        
        # Update requirements if needed
        if requirements_add:
            req_file = app_dir / "requirements.txt"
            if req_file.exists():
                with open(req_file, 'r') as f:
                    existing = f.read()
                
                # Add new requirements if not already present
                with open(req_file, 'a') as f:
                    for req in requirements_add:
                        if req not in existing:
                            f.write(f"\n{req}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error improving App {app_num:03d}: {e}")
        return False

# Define improvements for key apps
print("=" * 80)
print("üöÄ Master App Improvement Script")
print("=" * 80)
print()

improvements_applied = 0
total_apps = 0

# Skip app 1 and 2 (already done manually)
print("‚è≠Ô∏è  Skipping App 001 and 002 (already improved manually)")
print()

# App 003: Text Summarization
print("Improving App 003: Text Summarization...")
if create_improved_app(
    app_num=3,
    title="Text Summarization",
    emoji="üìÑ",
    use_case="Automatic document summarization",
    imports="""from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer
import nltk
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)""",
    function_code="""def process_text(text, sentences_count=3):
    '''Summarize text using TextRank algorithm'''
    try:
        parser = PlaintextParser.from_string(text, Tokenizer("english"))
        summarizer = TextRankSummarizer()
        
        # Generate summary
        summary_sentences = summarizer(parser.document, sentences_count)
        summary = ' '.join([str(sentence) for sentence in summary_sentences])
        
        # Calculate metrics
        original_sents = [s.strip() for s in text.split('.') if s.strip()]
        compression_ratio = (1 - len(summary) / max(len(text), 1)) * 100
        
        return {
            'text': text,
            'summary': summary,
            'original_length': len(text),
            'summary_length': len(summary),
            'original_sentences': len(original_sents),
            'summary_sentences': min(sentences_count, len(original_sents)),
            'compression_ratio': compression_ratio,
            'word_count': len(text.split()),
            'summary_word_count': len(summary.split())
        }
    except Exception as e:
        return {
            'text': text,
            'error': str(e),
            'summary': 'Error generating summary',
            'original_length': len(text),
            'word_count': len(text.split())
        }""",
    demo_texts=[
        "Artificial intelligence is revolutionizing technology. Machine learning enables computers to learn from data. Neural networks mimic human brain structure. Deep learning has achieved remarkable results in image and speech recognition. Natural language processing allows machines to understand human language. AI applications include autonomous vehicles, medical diagnosis, and virtual assistants. However, AI also raises ethical concerns about privacy, job displacement, and algorithmic bias. Responsible AI development requires careful consideration of these issues.",
        "Climate change is one of the greatest challenges facing humanity. Rising global temperatures are causing extreme weather events, melting ice caps, and rising sea levels. The primary cause is greenhouse gas emissions from human activities such as burning fossil fuels and deforestation. To address climate change, we must transition to renewable energy sources, improve energy efficiency, and protect natural ecosystems. International cooperation is essential to limit global warming and adapt to unavoidable changes. Individual actions, such as reducing consumption and supporting sustainable practices, also play an important role."
    ]
):
    improvements_applied += 1
total_apps += 1

# App 004: Language Detection
print("Improving App 004: Language Detection...")
if create_improved_app(
    app_num=4,
    title="Language Detection",
    emoji="üåç",
    use_case="Multi-language identification",
    imports="""from langdetect import detect, detect_langs, DetectorFactory
DetectorFactory.seed = 0  # For consistent results""",
    function_code="""def process_text(text):
    '''Detect the language of text'''
    try:
        lang_code = detect(text)
        lang_probs = detect_langs(text)
        
        # Language name mapping
        lang_names = {
            'en': 'English', 'es': 'Spanish', 'fr': 'French', 'de': 'German',
            'it': 'Italian', 'pt': 'Portuguese', 'ru': 'Russian', 'ja': 'Japanese',
            'zh-cn': 'Chinese', 'ar': 'Arabic', 'hi': 'Hindi', 'ko': 'Korean',
            'nl': 'Dutch', 'sv': 'Swedish', 'no': 'Norwegian', 'da': 'Danish'
        }
        
        detected_lang = lang_names.get(lang_code, lang_code.upper())
        
        # Parse probabilities
        prob_dict = {}
        for lp in lang_probs:
            parts = str(lp).split(':')
            if len(parts) == 2:
                prob_dict[parts[0]] = float(parts[1])
        
        top_prob = max(prob_dict.values()) if prob_dict else 0.0
        
        return {
            'text': text,
            'language_code': lang_code,
            'language_name': detected_lang,
            'confidence': top_prob,
            'text_length': len(text),
            'word_count': len(text.split()),
            'all_probabilities': str(prob_dict)
        }
    except Exception as e:
        return {
            'text': text,
            'error': str(e),
            'language_code': 'unknown',
            'language_name': 'Unknown',
            'confidence': 0.0,
            'word_count': len(text.split())
        }""",
    demo_texts=[
        "Hello, how are you today? This is a sample text in English.",
        "Bonjour, comment allez-vous? Ceci est un exemple de texte en fran√ßais.",
        "Hola, ¬øc√≥mo est√°s? Este es un texto de ejemplo en espa√±ol.",
        "Hallo, wie geht es dir? Dies ist ein Beispieltext auf Deutsch.",
        "Ciao, come stai? Questo √® un testo di esempio in italiano."
    ],
    requirements_add=["langdetect==1.0.9"]
):
    improvements_applied += 1
total_apps += 1

print()
print("=" * 80)
print(f"üìä Summary:")
print(f"   ‚úÖ Apps Improved: {improvements_applied}")
print(f"   üìù Apps Processed: {total_apps}")
print("=" * 80)
print()
print("üí° Note: Apps 1-4 have been improved with proper NLP functionality.")
print("   Continue this pattern for remaining apps or run selective improvements.")
